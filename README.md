# Etcd Filesystem Delay Reproduction (with charybdefs)

This README explains **how to reproduce leader‐WAL filesystem delay experiments on an etcd cluster** using Docker and **charybdefs** (a FUSE FS with RPC-controlled fault injection). It includes full setup from scratch, all commands, what we changed in the code, how to re-run, machine requirements, dependencies, and debugging notes.

---

## 1) What you will build

- A 3-node etcd cluster in Docker (`etcd0`, `etcd1`, `etcd2`).
- The data directory of **one node** (the target leader, e.g., `etcd2`) is **mounted via charybdefs** so we can inject **WAL-only** delays via Thrift RPC.
- An orchestrator script `run_etcd_fsdelay.sh` that:
  - Forces leadership to the slow node (or uses current leader),
  - Injects/clears the FUSE delay,
  - Runs a small PUT workload against the current leader,
  - Saves three CSVs per run: `per_op_latency.csv`, `throughput_per_sec.csv`, `latency_per_sec.csv`.
- Optional sweep scripts and plotting utilities.

---

## 2) Machine requirements

**Host OS:** Ubuntu 20.04+ (or any Linux with FUSE 2.9+)  
**CPU/RAM:** ≥ 4 vCPU, ≥ 8 GB RAM recommended  
**Disk:** SSD recommended (≥ 20 GB free)  
**Network:** Internet for `git clone` and Docker images  
**Privileged ops:** You need `sudo` for FUSE mount and some dev packages

---

## 3) Dependencies

### System packages
```bash
sudo apt-get update
sudo apt-get install -y \
  build-essential cmake pkg-config git \
  fuse libfuse-dev \
  thrift-compiler libthrift-dev \
  python3 python3-pip \
  docker.io docker-compose-plugin \
  jq time
```

> If your distro ships Docker differently, install Docker Engine + Compose Plugin from your distro or Docker docs.

### Python packages (host)
We only need the Python Thrift runtime. Ubuntu usually includes it in the system packages above; if not:
```bash
pip3 install thrift
```

---

## 4) Get the sources

Assume your home is `/home/cc` and the project lives in `~/cassandra-demo`.

```bash
cd ~
git clone https://github.com/haryadi-gunawi/charybdefs.git   # or your fork
git clone https://github.com/your-org/cassandra-demo.git     # your repo; contains scripts & compose
cd cassandra-demo
```

> If you already have the repo (as in our runs), just reuse it.

---

## 5) Build and run charybdefs

### Build
```bash
cd ~/charybdefs
mkdir -p build && cd build
cmake ..
make -j
# charybdefs binary will be in this build dir or project root depending on repo version
```

### Prepare mountpoints
We’ll have a “real” directory where etcd writes (`/data/raw/etcd2`) and a **FUSE mount** that shadows it (`/mnt/slowfs/etcd2`).

```bash
sudo mkdir -p /data/raw/etcd{0,1,2}
sudo mkdir -p /mnt/slowfs/etcd2
sudo chown -R $USER:$USER /data/raw /mnt/slowfs
```

### Run charybdefs with subdir=real-path
From **one terminal**:
```bash
# replace PATH_TO_BINARY below if your build path differs
sudo ~/charybdefs/charybdefs -f /mnt/slowfs/etcd2 \
  -omodules=subdir,subdir=/data/raw/etcd2 \
  -oallow_other,nonempty,debug
```
Keep this terminal open; it prints FUSE ops and RPC log lines.

**Verify the mount**
```bash
findmnt -T /mnt/slowfs/etcd2 -o TARGET,FSTYPE,SOURCE,OPTIONS
sudo ss -ltnp | awk '$4 ~ /:9090/'   # charybdefs RPC listens on :9090
```

> If `:9090` is not listening, ensure your charybdefs build exposes RPC. Our runs used the default that listens on 9090.

---

## 6) Bring up etcd (Docker)

Inside the project:
```bash
cd ~/cassandra-demo
docker compose -f docker-compose-etcd.yml up -d
docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Mounts}}'
```

> The compose file must mount `etcd2`’s data dir to `/mnt/slowfs/etcd2` on the **host** to ensure WAL I/O flows through charybdefs. Example volume mapping:
>
> - `etcd0` → host `/data/raw/etcd0`
> - `etcd1` → host `/data/raw/etcd1`
> - `etcd2` → host `/mnt/slowfs/etcd2`  (**the charybdefs mount**)

**Quick health check**
```bash
docker exec etcd0 etcdctl \
  --endpoints=http://etcd0:2379,http://etcd1:2379,http://etcd2:2379 \
  endpoint status -w table
```

---

## 7) Control charybdefs via Thrift (our CLI)

We provide a tiny CLI, **`charyb_fault.py`**, which talks to charybdefs RPC:
```bash
# set Thrift stub path (generated by charybdefs)
export PYTHONPATH="$HOME/charybdefs/gen-py:$PYTHONPATH"

# verify it runs
python3 charyb_fault.py clear
python3 charyb_fault.py delay \
  --delay-us 300000 \
  --methods open,create,write,write_buf,fsync,fdatasync,fsyncdir,flush \
  --regex '(^|.*/)member/wal/.*'
```

> If you see `ModuleNotFoundError: No module named 'server'`, the `PYTHONPATH` above is missing or wrong. See **Debug**.

---

## 8) The orchestrator: run_etcd_fsdelay.sh

This script does everything for one run (baseline or delay). Key features:
- Forces leadership to a chosen node (optional) using `etcdctl move-leader` or fallback restart.
- Injects a WAL-only delay (or clears it for baseline).
- Writes `OPS` keys to the **leader** endpoint.
- Produces:
  - `per_op_latency.csv` — raw per-op durations (seconds),
  - `throughput_per_sec.csv` — ops/sec buckets by elapsed time,
  - `latency_per_sec.csv` — avg latency/sec by elapsed time.

**Important environment variables**
- `LEADER_TARGET` — etcd0|etcd1|**etcd2** (slow node we mounted).
- `MODE` — `baseline` or `delay` (positional arg is also accepted).
- `WAL_DELAY_US` — injected delay in microseconds when `MODE=delay`.
- `OPS` — number of puts to issue (e.g., 200/1000).
- `VERIFY_DELAY=1` — run a host-side `fsync` sanity check.
- `WAL_METHODS` — comma list of methods to delay (defaults to `open,create,write,write_buf,fsync,fdatasync,fsyncdir,flush`).
- `WAL_REGEX` — path regex; our WAL is `(^|.*/)member/wal/.*`.

**Examples**
```bash
# Baseline (no delay), forcing leader to etcd2
LEADER_TARGET=etcd2 OPS=200 ./run_etcd_fsdelay.sh baseline

# Delay 300ms on leader’s WAL
LEADER_TARGET=etcd2 OPS=200 WAL_DELAY_US=300000 VERIFY_DELAY=1 ./run_etcd_fsdelay.sh delay
```

**Paper-like sweep**
```bash
# microseconds: 1, 10, 100, 10ms, 100ms, 1s
for d in 1 10 100 10000 100000 1000000; do
  LEADER_TARGET=etcd2 OPS=1000 WAL_DELAY_US=$d VERIFY_DELAY=1 ./run_etcd_fsdelay.sh delay
done

# custom: 1ms, 5ms, 10ms, 25ms, 50ms, 75ms, 100ms
for d in 1000 5000 10000 25000 50000 75000 100000; do
  LEADER_TARGET=etcd2 OPS=1000 WAL_DELAY_US=$d VERIFY_DELAY=1 ./run_etcd_fsdelay.sh delay
done
```

**Result layout**
```
results/
  20250809_222355_etcd_fsdelay_delay_etcd2_300000us/
    per_op_latency.csv
    throughput_per_sec.csv
    latency_per_sec.csv
```

---

## 9) Plotting the results (optional)

You can use a local notebook or a minimal Python script like this:

```python
# plot_results.py
import glob, os, csv, matplotlib.pyplot as plt

base = "results"
runs = sorted(glob.glob(os.path.join(base, "*_etcd_fsdelay_*")))

# 1) CDF from per_op_latency.csv
plt.figure()
for r in runs:
    lab = os.path.basename(r).split("_")[-1]  # e.g., 300000us
    xs = []
    with open(os.path.join(r, "per_op_latency.csv")) as f:
        next(f)  # header
        for line in f:
            _, s = line.strip().split(",")
            if s != "NaN":
                xs.append(float(s) * 1000)  # ms
    xs.sort()
    if not xs: 
        continue
    ys = [i/len(xs) for i in range(1, len(xs)+1)]
    plt.step(xs, ys, where="post", label=lab)
plt.xlabel("Latency (ms)"); plt.ylabel("CDF"); plt.title("Latency CDF by Delay"); plt.legend(); plt.grid(True)
plt.show()

# 2) Throughput vs time from throughput_per_sec.csv
plt.figure()
for r in runs:
    lab = os.path.basename(r).split("_")[-1]
    ts, ops = [], []
    with open(os.path.join(r, "throughput_per_sec.csv")) as f:
        next(f)
        for line in f:
            t, n = line.strip().split(",")
            ts.append(int(t)); ops.append(int(n))
    plt.plot(ts, ops, label=lab)
plt.xlabel("Time (s)"); plt.ylabel("Throughput (ops/s)"); plt.title("Throughput vs Time"); plt.legend(); plt.grid(True)
plt.show()

# 3) Latency vs time from latency_per_sec.csv
plt.figure()
for r in runs:
    lab = os.path.basename(r).split("_")[-1]
    ts, ls = [], []
    with open(os.path.join(r, "latency_per_sec.csv")) as f:
        next(f)
        for line in f:
            t, a = line.strip().split(",")
            ts.append(int(t)); ls.append(float(a) * 1000)  # ms
    plt.plot(ts, ls, label=lab)
plt.xlabel("Time (s)"); plt.ylabel("Latency (ms)"); plt.title("Latency vs Time"); plt.legend(); plt.grid(True)
plt.show()
```

Run it:
```bash
python3 plot_results.py
```

---

## 10) Code we added/modified

### 10.1 `charyb_fault.py` (RPC client for charybdefs)
Key points:
- Uses Thrift stubs from `~/charybdefs/gen-py` (`server` module).
- Two subcommands:
  - `clear` → `clear_all_faults()`
  - `delay` → `set_fault(..., prob_permil=1000, regex, delay_us)`
- Default regex matches WAL: `(^|.*/)member/wal/.*`.
- Fixes the `argparse` subparser bug by using `set_defaults(func=...)`.

```python
#!/usr/bin/env python3
import argparse, sys
from thrift.transport import TSocket, TTransport
from thrift.protocol import TBinaryProtocol
from server import server as srv

def connect(host, port):
    s = TSocket.TSocket(host, int(port))
    t = TTransport.TBufferedTransport(s)
    p = TBinaryProtocol.TBinaryProtocol(t)
    t.open()
    return srv.Client(p), t

def do_clear(args):
    c, t = connect(args.host, args.port)
    c.clear_all_faults()
    t.close()
    print("Charybdefs: CLEARED all faults")

def do_delay(args):
    methods = [m.strip() for m in args.methods.split(",") if m.strip()]
    c, t = connect(args.host, args.port)
    c.set_fault(methods, False, 0, args.prob_permil, args.regex, False, args.delay_us, False)
    t.close()
    print(f"Charybdefs: injected delay {args.delay_us} us methods={methods} prob={args.prob_permil}/1000 regex='{args.regex}'")

def main():
    ap = argparse.ArgumentParser(description="Control charybdefs faults via Thrift")
    ap.add_argument("--host", default="127.0.0.1")
    ap.add_argument("--port", default=9090, type=int)

    sp = ap.add_subparsers(dest="cmd", required=True)

    p0 = sp.add_parser("clear", help="clear all faults")
    p0.set_defaults(func=do_clear)

    p1 = sp.add_parser("delay", help="inject a delay fault")
    p1.add_argument("--methods", default="fsync,fdatasync,fsyncdir")
    p1.add_argument("--regex", default=r"(^|.*/)member/wal/.*")
    p1.add_argument("--prob-permil", default=1000, type=int)  # 1000 = 100%
    p1.add_argument("--delay-us", required=True, type=int)
    p1.set_defaults(func=do_delay)

    args = ap.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()
```

### 10.2 `run_etcd_fsdelay.sh` (one-shot experiment runner)
Highlights:
- Robust leader discovery with JSON scrubbing (some etcdctl builds print warnings before JSON).
- `ensure_leader_target` tries `move-leader`, falls back to restart if needed.
- WAL-only delay injection using `charyb_fault.py` with user-provided `WAL_METHODS` and `WAL_REGEX`.
- Per-op timing via `/usr/bin/time -f %e` and aggregation into CSVs.
- Optional host-side `fsync` verification.

> The full script in this repo matches the one used in our successful runs.

---

## 11) Re-run guide (short version)

```bash
# 0) Start charybdefs (terminal A)
sudo ~/charybdefs/charybdefs -f /mnt/slowfs/etcd2 \
  -omodules=subdir,subdir=/data/raw/etcd2 -oallow_other,nonempty,debug

# 1) Start etcd (terminal B)
cd ~/cassandra-demo
docker compose -f docker-compose-etcd.yml up -d

# 2) Make sure Python can find Thrift stubs
export PYTHONPATH="$HOME/charybdefs/gen-py:$PYTHONPATH"

# 3) Baseline
LEADER_TARGET=etcd2 OPS=200 ./run_etcd_fsdelay.sh baseline

# 4) One delay run (e.g., 300ms)
LEADER_TARGET=etcd2 OPS=200 WAL_DELAY_US=300000 VERIFY_DELAY=1 ./run_etcd_fsdelay.sh delay

# 5) Sweep (custom)
for d in 1000 5000 10000 25000 50000 75000 100000; do
  LEADER_TARGET=etcd2 OPS=1000 WAL_DELAY_US=$d VERIFY_DELAY=1 ./run_etcd_fsdelay.sh delay
done
```

---

## 12) Export results to Windows (PowerShell)

```powershell
scp -C -i "C:\apendable\yizzz-mj-trace.pem" -r ^
  cc@192.5.86.216:/home/cc/cassandra-demo/results ^
  "C:\FileSystem\Newer"
```
> Avoid a trailing slash after the destination path; Windows `scp` treats it oddly.

---

## 13) Debugging notes (common issues)

**(A) `ModuleNotFoundError: No module named 'server'`**  
Cause: Python cannot find Thrift stubs.  
Fix:
```bash
export PYTHONPATH="$HOME/charybdefs/gen-py:$PYTHONPATH"
```

**(B) `Could not connect to any of [('127.0.0.1', 9090)]`**  
Cause: charybdefs RPC not running or blocked.  
Fix:
```bash
sudo ss -ltnp | awk '$4 ~ /:9090/'
# If empty: restart charybdefs; ensure the binary you run exposes RPC.
```

**(C) Leader forcing is flaky / JSON errors**  
Symptom: etcdctl prints “unrecognized environment variable ETCDCTL_API=3” before JSON; parser fails.  
Fix: Our script strips non-JSON lines automatically. You can also run:
```bash
docker exec etcd0 etcdctl --endpoints=http://etcd0:2379 member list -w json | jq .
```

**(D) Delay matches wrong paths**  
Symptom: Delay doesn’t show up on host `fsync` test.  
Fix: Ensure the regex is `(^|.*/)member/wal/.*` and the slow node’s data dir is the **charybdefs mount**.

**(E) Distroless etcd images and `/bin/sh`**  
Some images don’t include an interactive shell. Our script uses `etcdctl` directly and times outside the container, so it’s fine. If you need a shell, use another container or `--entrypoint` trick.

**(F) Permissions / FUSE**  
If `allow_other` fails, ensure `/etc/fuse.conf` contains `user_allow_other`.

---

## 14) What the numbers mean

- `per_op_latency.csv`: two columns: `op,seconds`. Use this for CDFs and per-op lines.
- `throughput_per_sec.csv`: `sec,ops` counted by elapsed second.
- `latency_per_sec.csv`: `sec,avg_latency_s` computed from ops that completed in that second.

---

## 15) Clean up

```bash
# stop etcd
docker compose -f docker-compose-etcd.yml down -v

# unmount charybdefs
sudo umount /mnt/slowfs/etcd2
```

---

## 16) Appendix: quick host-side delay check

```bash
python3 - <<'PY'
import os, time
p="/mnt/slowfs/etcd2/member/wal/_probe"
t=time.time()
with open(p,"wb") as f:
    f.write(b"x"*4096); os.fsync(f.fileno())
dt=time.time()-t
os.remove(p)
print(f"fsync elapsed ~{dt:.3f}s")
PY
```

If you injected `--delay-us 300000`, the elapsed time should be roughly `~0.300s` plus small overhead.

---

Happy experimenting!
